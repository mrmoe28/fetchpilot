# FetchPilot Features Documentation

Complete feature reference for FetchPilot - Autonomous Web Intelligence Agent

## Table of Contents

1. [Core Features](#core-features)
2. [LLM-Guided Intelligence](#llm-guided-intelligence)
3. [Scraping Capabilities](#scraping-capabilities)
4. [API Reference](#api-reference)
5. [User Interface](#user-interface)
6. [Browser Worker](#browser-worker)
7. [Configuration](#configuration)
8. [Data Extraction](#data-extraction)
9. [Troubleshooting](#troubleshooting)

---

## Core Features

### 1. Autonomous Web Scraping
**Purpose**: Automatically extract structured product data from any e-commerce website without hardcoded selectors.

**How It Works**:
- Queue-based breadth-first crawling starting from a seed URL
- LLM analyzes each page and generates optimal extraction strategy
- Automatic pagination discovery and following
- Product deduplication using composite `url::title` keys
- Configurable stop criteria (min products, max pages)

**Implementation**: `lib/agent.ts:7-63` (`scrapeProducts()`)

**Example**:
```typescript
const products = await scrapeProducts(
  "https://example.com/products",
  "Extract all product listings with prices",
  {
    anthropicKey: "sk-ant-...",
    maxTotalPages: 20
  }
);
```

**Dependencies**: Anthropic API key, optional browser worker

---

### 2. HTTP-First Architecture
**Purpose**: Minimize costs and latency by preferring lightweight HTTP requests over browser automation.

**How It Works**:
- Initial page fetch via standard HTTP (`HttpFetcher`)
- Extracts DOM signals: link count, image count, JSON-LD presence
- LLM decides whether HTTP is sufficient or browser needed
- Only launches browser for JS-heavy/lazy-loaded content

**Implementation**: `lib/tools.ts:10-23` (`HttpFetcher.fetch()`)

**Configuration**:
```typescript
// HTTP-only mode (no browser worker)
const products = await scrapeProducts(url, goal, {
  anthropicKey: "...",
  browser: undefined
});
```

**Performance**: 10-50x faster than browser-based scraping for server-rendered content

---

### 3. Adaptive Strategy Selection
**Purpose**: Automatically choose optimal parsing approach based on page structure.

**Strategies**:
- **JSONLD**: Parse structured data from `<script type="application/ld+json">` tags
- **CSS**: Use CSS selectors generated by LLM
- **XPATH**: (Schema supports, not yet implemented)
- **HYBRID**: Combine JSON-LD + CSS for maximum coverage

**How It Works**:
1. LLM receives page observation (HTML sample, DOM signals)
2. Analyzes content structure and metadata availability
3. Returns `AgentDecision` with recommended strategy
4. System executes chosen approach(es)

**Implementation**:
- Decision logic: `lib/brain.ts:3-43` (`askClaudeForDecision()`)
- JSON-LD parser: `lib/agent.ts:65-92` (`parseJsonLd()`)
- CSS extraction: `lib/tools.ts:43-61` (`extractProductsHTML()`)

**Example Decision**:
```json
{
  "rationale": "Page has JSON-LD product data; fallback CSS for missed items",
  "actions": [{
    "mode": "HTTP",
    "parseStrategy": "HYBRID",
    "selectors": {
      "item": ".product-card",
      "link": "a.product-link",
      "title": "h3.title",
      "price": ".price"
    }
  }]
}
```

---

## LLM-Guided Intelligence

### 4. Claude-Powered Decision Engine
**Purpose**: Replace brittle hardcoded selectors with intelligent, adaptive strategies.

**Model**: `claude-3-5-sonnet-latest` (Anthropic API)

**Decision Process**:
1. **Input**: Page observation + user goal
   - URL, HTTP status, HTML snippet
   - DOM signals (link/image counts, JSON-LD presence)
   - User-provided goal (e.g., "Extract product listings")

2. **Output**: Structured `AgentDecision` conforming to Zod schema
   - Mode: HTTP vs BROWSER
   - Parse strategy: CSS/JSONLD/HYBRID
   - CSS selectors for product extraction
   - Pagination config (link selector, max pages)
   - Anti-lazy loading settings (scroll times, wait delays)
   - Retry logic and stop criteria

3. **Fallback**: If LLM response invalid, uses sensible defaults

**Implementation**: `lib/brain.ts:3-43`

**Prompt Template**:
```typescript
System: "ROLE: Scraping Strategist. Return STRICT JSON conforming to AgentDecision schema..."
User: "Goal: ${goal}\nObservation: ${JSON.stringify(obs)}\nSchema keys: AgentDecision { ... }"
```

**Cost**: ~$0.003-0.01 per page analyzed (based on Claude pricing)

**Accuracy Tuning**:
- Increase `max_tokens` (currently 1200) for more detailed reasoning
- Add few-shot examples to system prompt
- Provide more granular DOM signals in observation

---

### 5. Self-Correcting Pagination
**Purpose**: Automatically discover and follow pagination without manual configuration.

**Supported Types**:
- **LINK**: Follow `<a rel="next">` or `.pagination .next` links
- **BUTTON**: Click "Load More" buttons (requires browser mode)
- **SCROLL**: Infinite scroll detection (requires browser mode)
- **PARAMS**: URL parameter manipulation (e.g., `?page=2`)
- **NONE**: Single-page scraping

**How It Works**:
1. LLM analyzes pagination pattern from first page
2. Generates selector or parameter strategy
3. System discovers next page URLs using `findNextLinks()`
4. Adds to crawl queue with duplicate detection

**Implementation**:
- Link discovery: `lib/agent.ts:94-105` (`findNextLinks()`)
- Queue management: `lib/agent.ts:15-60` (while loop in `scrapeProducts()`)

**Example**:
```json
{
  "pagination": {
    "type": "LINK",
    "selector": "a.next-page, .pagination a:contains('Next')",
    "maxPages": 10
  }
}
```

**Limits**: `maxPages` prevents infinite loops, `visited` Set prevents cycles

---

## Scraping Capabilities

### 6. Product Data Extraction
**Purpose**: Extract comprehensive product information from e-commerce sites.

**Supported Fields**:
- `url` (required): Product detail page URL
- `title` (required): Product name/title
- `price` (optional): Price as string (preserves currency formatting)
- `image` (optional): Primary product image URL
- `inStock` (optional): Boolean availability status
- `sku` (optional): Product SKU/identifier
- `currency` (optional): Currency code (e.g., USD, EUR)
- `breadcrumbs` (optional): Category breadcrumb trail
- `extra` (optional): Arbitrary additional metadata

**Schema**: `lib/schemas.ts:3-13` (Zod `Product` schema)

**Extraction Methods**:

**JSON-LD Parsing**:
```typescript
// Parses <script type="application/ld+json"> tags
// Supports @graph structures and Product schema
const products = parseJsonLd(html);
```

**CSS Selector Parsing**:
```typescript
// Uses Cheerio with LLM-generated selectors
const products = extractProductsHTML(html, baseUrl, {
  item: ".product-card",
  link: "a[href]",
  title: "h2.product-title",
  price: ".price",
  image: "img"
});
```

**Image Handling**:
- Checks `src`, `data-src`, and `srcset` attributes
- Resolves relative URLs to absolute
- Implementation: `lib/tools.ts:56`

---

### 7. Deduplication & Merging
**Purpose**: Eliminate duplicate products when scraping multiple pages.

**Strategy**: Composite key `url::title`

**How It Works**:
```typescript
function merge(dst: Product[], src: Product[]) {
  const seen = new Set(dst.map(p => `${p.url}::${p.title}`));
  for (const p of src) {
    const key = `${p.url}::${p.title}`;
    if (!seen.has(key)) {
      dst.push(p);
      seen.add(key);
    }
  }
}
```

**Implementation**: `lib/agent.ts:107-113`

**Edge Cases**:
- Same product on different URLs → treated as separate
- Variant products (sizes/colors) → deduplicated if same URL+title
- Missing URLs → may cause false duplicates

---

### 8. Anti-Lazy Loading
**Purpose**: Handle infinite scroll, "Load More" buttons, and delayed content rendering.

**Features**:
- Configurable scroll repetitions
- Wait delays between scrolls
- Maximum scroll limit to prevent infinite loops

**Configuration**:
```json
{
  "antiLazy": {
    "scroll": true,
    "waitMs": 800,
    "maxScrolls": 6
  }
}
```

**Browser Implementation**: `worker/browser-worker.ts:16-21`
```typescript
if (body.scroll) {
  for (let i = 0; i < body.scroll.times; i++) {
    await page.evaluate(() => window.scrollTo(0, document.body.scrollHeight));
    await page.waitForTimeout(body.scroll.waitMs || 800);
  }
}
```

**Requires**: Browser worker (Playwright)

---

## API Reference

### 9. REST API Endpoint

**Endpoint**: `POST /api/scrape`

**Request**:
```json
{
  "url": "https://example.com/products",
  "goal": "Extract all product listings with prices and availability"
}
```

**Request Schema** (`app/api/scrape/route.ts:8-11`):
- `url`: string (required, must be valid URL)
- `goal`: string (optional, defaults to "Extract product cards and canonical links")

**Response** (Success - 200):
```json
{
  "products": [
    {
      "url": "https://example.com/product/1",
      "title": "Product Name",
      "price": "$29.99",
      "image": "https://cdn.example.com/img.jpg",
      "inStock": true,
      "currency": "USD"
    }
  ],
  "logs": [
    "API: returned 15 items"
  ]
}
```

**Response** (Error - 400):
```
Bad Request
```
(Plain text error message)

**Configuration** (`app/api/scrape/route.ts:17-20`):
- Runtime: `nodejs` (not Edge)
- Max pages: 12
- Browser: Auto-detected from env vars

**Example Usage**:
```bash
curl -X POST http://localhost:3000/api/scrape \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com/shop",
    "goal": "Extract product titles, prices, and stock status"
  }'
```

---

## User Interface

### 10. Scraper Form
**Purpose**: Input interface for scraping jobs.

**Location**: `components/scraper-form.tsx`

**Fields**:
- **Start URL**: Target website URL (required)
- **Goal**: Natural language description of extraction objective (optional, pre-filled with default)

**Features**:
- Real-time input validation
- Loading state with disabled submit button
- Default goal: "Extract product cards and canonical links"
- Responsive grid layout (2 columns on desktop)

**Badges**: "HTTP-first", "LLM-guided", "Self-correcting"

**Implementation** (`components/scraper-form.tsx:9-35`):
```tsx
<ScraperForm
  onSubmit={(data) => runScrape(data)}
  loading={isRunning}
/>
```

---

### 11. Results Table
**Purpose**: Display extracted product data in tabular format.

**Columns**:
- Title (text)
- URL (clickable link, opens in new tab)
- Price (formatted string or "-" if missing)
- Image (thumbnail preview, 40px height, rounded)
- In Stock (Yes/No/"-")

**Features**:
- Horizontal scroll for overflow
- Empty state message when no results
- Automatic image rendering with alt text
- External link styling (blue underline)

**Implementation**: `components/results-table.tsx:1-27`

**Styling**:
- Rounded table design
- Responsive overflow handling
- Custom FetchPilot color scheme

---

### 12. Real-Time Log Viewer
**Purpose**: Stream scraping progress and debug information.

**Features**:
- Monospace font for readability
- Auto-scroll to latest logs
- Fixed height container (360px) with overflow scroll
- Light gray background

**Log Format**:
- `▶ Starting FetchPilot...` - Job started
- `Fetch HTTP: {url}` - HTTP request initiated
- `Decision: {rationale}; actions={count}` - LLM decision received
- `→ Browser mode for {url}` - Switching to browser
- `Parsed +{count} items (total {total})` - Extraction progress
- `✔ Done` - Job completed
- `✖ Error: {message}` - Error occurred

**Implementation**: `components/log-view.tsx:1-7`

**Example Output**:
```
▶ Starting FetchPilot...
Fetch HTTP: https://example.com/products
Decision: Page has JSON-LD data; actions=1
Parsed +12 items (total 12)
Parsed +15 items (total 27)
✔ Done
```

---

### 13. Responsive Layout
**Purpose**: Modern, mobile-friendly UI with shadcn-inspired components.

**Structure** (`app/page.tsx:39-59`):
- Form card (full width)
- Two-column grid for results + logs (stacks on mobile)
- Max-width container (6xl = 1152px)
- Rounded cards with soft shadows

**Design System**:
- Custom color: `fetchpilot.primary` (brand blue)
- Rounded corners: 2xl (16px)
- Soft shadows on cards
- Border-less card design

**Components**:
- Card, CardContent (`components/ui/card.tsx`)
- Input, Textarea (`components/ui/input.tsx`, `components/ui/textarea.tsx`)
- Button (`components/ui/button.tsx`)
- Table (`components/ui/table.tsx`)
- Badge, Label (`components/ui/badge.tsx`, `components/ui/label.tsx`)

---

## Browser Worker

### 14. Playwright Service
**Purpose**: Optional headless browser service for JavaScript-rendered content.

**Architecture**: Standalone HTTP server (separate from Next.js app)

**Port**: 8787

**Endpoint**: `POST /openPage`

**Request**:
```json
{
  "url": "https://example.com",
  "scroll": {
    "times": 5,
    "waitMs": 1000
  },
  "waitMs": 2000
}
```

**Response**:
```json
{
  "url": "https://example.com/final",
  "status": 200,
  "html": "<html>...</html>",
  "domSignals": {
    "numLinks": 145,
    "numImages": 32,
    "scrollHeight": 5420
  }
}
```

**Implementation**: `worker/browser-worker.ts:1-48`

**Features**:
- Headless Chromium via Playwright
- Custom User-Agent: "Mozilla/5.0 (compatible; FetchPilot/1.0)"
- 45-second navigation timeout
- Infinite scroll simulation
- DOM statistics collection
- Automatic browser cleanup

**Lifecycle**:
```typescript
1. Launch browser → 2. Navigate to URL →
3. Wait for content → 4. Scroll if needed →
5. Extract HTML → 6. Collect stats → 7. Close browser
```

**Start Command**: `npm run worker`

**Memory**: ~100-200MB per request (browser instance not pooled)

---

### 15. Browser Client Integration
**Purpose**: Connect Next.js app to browser worker.

**Configuration**:
```env
BROWSER_WORKER_URL=http://localhost:8787/openPage
```

**Auto-Detection** (`lib/tools.ts:29-41`):
```typescript
export const ManagedBrowser: BrowserClient | undefined =
  process.env.BROWSER_WORKER_URL
    ? {
        openPage: async (input) => {
          const res = await fetch(process.env.BROWSER_WORKER_URL!, {
            method: "POST",
            headers: { "content-type": "application/json" },
            body: JSON.stringify(input)
          });
          return await res.json();
        }
      }
    : undefined;
```

**Fallback**: If `BROWSER_WORKER_URL` not set, system uses HTTP-only mode

**Usage in Agent** (`lib/agent.ts:27-34`):
```typescript
if (action.mode === "BROWSER" && (cfg.browser || ManagedBrowser)) {
  const browser = cfg.browser ?? ManagedBrowser!;
  logs.push(`→ Browser mode for ${url}`);
  res = await browser.openPage({
    url,
    scroll: action.antiLazy.scroll ? {
      times: action.antiLazy.maxScrolls,
      waitMs: action.antiLazy.waitMs
    } : undefined
  });
}
```

---

## Configuration

### 16. Environment Variables

**Required**:
```env
ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```
- **Purpose**: Authenticate with Claude API for LLM decisions
- **Where Used**: `lib/brain.ts:14-22`
- **Validation**: `lib/agent.ts:8` throws error if missing

**Optional**:
```env
BROWSER_WORKER_URL=http://localhost:8787/openPage
```
- **Purpose**: Enable browser mode for JS-heavy sites
- **Default**: `undefined` (HTTP-only mode)
- **Where Used**: `lib/tools.ts:29`

```env
PLAYWRIGHT_ENABLED=false
```
- **Purpose**: Documented in `.env.example` but not currently used in code
- **Future Use**: May control browser mode availability

**Setup**:
1. `cp .env.example .env.local`
2. Fill in `ANTHROPIC_API_KEY`
3. Optionally set `BROWSER_WORKER_URL` if running worker

---

### 17. Scraping Limits

**Max Total Pages** (`app/api/scrape/route.ts:20`):
```typescript
maxTotalPages: 12
```
- Prevents infinite crawling
- Default: 12 pages
- Configurable per `scrapeProducts()` call

**Max Pages Per Pagination** (LLM decision):
```json
{
  "pagination": {
    "maxPages": 10
  }
}
```
- Limits pagination following
- Default: 10 (in fallback decision)

**Min Products Stop Criterion** (LLM decision):
```json
{
  "stopCriteria": {
    "minProducts": 10
  }
}
```
- Exits early when threshold met
- Implementation: `lib/agent.ts:56-58`

**Browser Timeout** (`worker/browser-worker.ts:14`):
```typescript
await page.goto(body.url, {
  waitUntil: "domcontentloaded",
  timeout: 45000
});
```
- 45 seconds per page navigation
- Prevents hanging on slow sites

---

### 18. Next.js Configuration

**File**: `next.config.mjs`

**Settings**:
```javascript
{
  experimental: {
    typedRoutes: true  // Type-safe navigation
  },
  images: {
    remotePatterns: [{
      protocol: 'https',
      hostname: '**'  // Allow all HTTPS images
    }]
  }
}
```

**Image Optimization**: Supports external images from any HTTPS domain (necessary for scraped product images)

**Typed Routes**: Enables TypeScript route validation with `next/link`

---

## Data Extraction

### 19. JSON-LD Structured Data
**Purpose**: Parse schema.org Product markup from `<script type="application/ld+json">` tags.

**Advantages**:
- More reliable than CSS selectors
- Standardized schema
- Includes rich metadata (currency, availability)

**Supported Patterns**:
- Single Product object
- Array of Products
- `@graph` structures with nested Products

**Mapping**:
```typescript
{
  url: offer?.url || node.url,
  title: node.name,
  price: offer?.price,
  image: Array.isArray(node.image) ? node.image[0] : node.image,
  currency: offer?.priceCurrency
}
```

**Implementation**: `lib/agent.ts:65-92`

**Example Source**:
```html
<script type="application/ld+json">
{
  "@type": "Product",
  "name": "Wireless Headphones",
  "image": "https://cdn.example.com/headphones.jpg",
  "offers": {
    "@type": "Offer",
    "price": "99.99",
    "priceCurrency": "USD",
    "url": "https://example.com/headphones"
  }
}
</script>
```

**Fallback**: If no Products found in JSON-LD, returns empty array

---

### 20. CSS Selector Extraction
**Purpose**: Extract products using LLM-generated CSS selectors when JSON-LD unavailable.

**How It Works**:
1. LLM analyzes HTML structure
2. Generates selectors for:
   - `item`: Product container element
   - `link`: Product URL anchor tag
   - `title`: Product name element
   - `price`: Price text element
   - `image`: Product image tag

3. Cheerio iterates over item containers
4. Extracts child elements using provided selectors

**Implementation**: `lib/tools.ts:43-61`

**Example Selectors**:
```json
{
  "item": ".product-card, [data-product], li.product-item",
  "link": "a.product-link, a[href*='/product/']",
  "title": "h2.title, h3.product-name, .product-title",
  "price": ".price, .product-price, [class*='price']",
  "image": "img.product-img, img[data-src]"
}
```

**Features**:
- Multiple selector fallbacks (comma-separated)
- Relative URL resolution
- Image srcset parsing
- Text trimming

---

### 21. URL Resolution
**Purpose**: Convert relative URLs to absolute for proper link following.

**Implementation**:
```typescript
function toAbs(href?: string) {
  if (!href) return "";
  try {
    return new URL(href, baseUrl).toString();
  } catch {
    return href;
  }
}
```

**Handles**:
- Relative paths: `/products/123` → `https://example.com/products/123`
- Protocol-relative: `//cdn.example.com/img.jpg` → `https://cdn.example.com/img.jpg`
- Absolute URLs: Pass through unchanged
- Invalid URLs: Return original string

**Location**: `lib/tools.ts:46-49`

---

## Troubleshooting

### Common Issues

#### 1. "Missing ANTHROPIC_API_KEY" Error
**Cause**: Environment variable not set

**Solution**:
```bash
cp .env.example .env.local
# Edit .env.local and add:
ANTHROPIC_API_KEY=sk-ant-your-key-here
```

**Verification**: Check `lib/agent.ts:8` - should not throw

---

#### 2. No Products Extracted
**Possible Causes**:
- Site requires JavaScript rendering
- CSS selectors don't match page structure
- Products behind authentication/paywall
- Rate limiting or bot detection

**Debugging**:
1. Check logs for "Parsed +0 items"
2. Verify LLM decision in logs (mode, selectors)
3. Inspect HTML structure manually
4. Try enabling browser worker if in HTTP mode
5. Check if site has JSON-LD data (view page source, search for `application/ld+json`)

**Solutions**:
- Enable browser worker: `npm run worker` + set `BROWSER_WORKER_URL`
- Adjust goal to be more specific about desired data
- Check if site blocks scrapers (User-Agent filtering)

---

#### 3. Browser Worker Connection Failed
**Error**: "Browser worker error"

**Cause**: Worker not running or wrong URL

**Solution**:
```bash
# Terminal 1: Start worker
npm run worker

# Terminal 2: Verify worker responds
curl -X POST http://localhost:8787/openPage \
  -H "Content-Type: application/json" \
  -d '{"url":"https://example.com"}'

# Check .env.local has correct URL
BROWSER_WORKER_URL=http://localhost:8787/openPage
```

**Verification**: `lib/tools.ts:32-38` should successfully fetch

---

#### 4. Slow Scraping Performance
**Causes**:
- Browser mode for every page
- Large `maxTotalPages` value
- Excessive scroll repetitions
- LLM API latency

**Solutions**:
- Reduce `maxTotalPages` in `app/api/scrape/route.ts:20`
- Use HTTP mode when possible (disable browser worker)
- Optimize LLM prompts for faster responses
- Implement caching for repeated URLs

**Performance Benchmarks**:
- HTTP mode: ~1-2 seconds per page
- Browser mode: ~5-15 seconds per page (depending on scroll)
- LLM decision: ~1-3 seconds per decision

---

#### 5. Invalid JSON from LLM
**Error**: Fallback decision used

**Cause**: LLM returned malformed JSON or unexpected format

**Debug**:
1. Check `lib/brain.ts:24-28` - logs raw LLM response
2. Inspect Anthropic API status (rate limits, outages)
3. Verify `max_tokens` (1200) is sufficient

**Fallback Behavior**: Uses sensible defaults (HTTP + HYBRID + common selectors)

**Prevention**:
- Improve system prompt with examples
- Increase `max_tokens` for complex pages
- Add JSON schema validation hints in prompt

---

#### 6. Pagination Not Following
**Causes**:
- Selector doesn't match pagination links
- JavaScript-based pagination (requires browser)
- URL already visited (deduplication)

**Debugging**:
1. Check LLM decision for `pagination.selector`
2. Inspect `findNextLinks()` output in logs
3. Verify `visited` Set not blocking valid URLs

**Solutions**:
- Adjust pagination selector in LLM prompt
- Use browser mode for JavaScript pagination
- Clear visited Set or adjust deduplication logic

**Implementation**: `lib/agent.ts:94-105`

---

#### 7. Memory Issues with Large Sites
**Symptoms**: High memory usage, crashes, slow response

**Causes**:
- Large HTML documents stored in memory
- Many products accumulated
- Browser instances not closing

**Solutions**:
- Reduce `maxTotalPages` limit
- Implement streaming/chunking for results
- Add browser instance pooling in worker
- Clear product array periodically

**Monitor**: Node.js memory with `process.memoryUsage()`

---

#### 8. Rate Limiting / Bot Detection
**Symptoms**: 429 errors, CAPTCHAs, empty responses

**Solutions**:
- Add delays between requests (not currently implemented)
- Rotate User-Agent strings
- Use residential proxies
- Implement exponential backoff retry logic
- Check robots.txt compliance

**Current User-Agent**: `lib/tools.ts:63`
```typescript
"Mozilla/5.0 (compatible; FetchPilot/1.0; +https://example.com/bot)"
```

**Future Enhancement**: Implement retry with jitter (schema supports, not yet in code)

---

## Advanced Usage

### Custom Browser Client

Inject custom browser implementation:

```typescript
import { scrapeProducts } from "@/lib/agent";

const customBrowser = {
  openPage: async (input) => {
    // Your custom Puppeteer/Playwright logic
    return {
      url: input.url,
      html: "...",
      domSignals: { numLinks: 100, numImages: 20 }
    };
  }
};

const products = await scrapeProducts(url, goal, {
  anthropicKey: "...",
  browser: customBrowser,
  maxTotalPages: 50
});
```

### Logging Integration

Capture detailed logs:

```typescript
const logs: string[] = [];

const products = await scrapeProducts(url, goal, {
  anthropicKey: "...",
  logs // Pass by reference, will be populated
});

console.log(logs); // All scraping events
```

### Schema Extensions

Add custom fields to Product schema:

1. Update `lib/schemas.ts:3-13`:
```typescript
export const Product = z.object({
  url: z.string().url(),
  title: z.string(),
  // ... existing fields
  rating: z.number().optional(),
  reviewCount: z.number().optional(),
  category: z.string().optional()
});
```

2. Update extraction logic in `lib/tools.ts` and `lib/agent.ts`

3. Update LLM prompt in `lib/brain.ts` to request new fields

---

## Performance Optimization

### Recommendations

1. **Use HTTP mode when possible**: 10-50x faster than browser
2. **Implement result streaming**: Don't wait for all pages
3. **Cache LLM decisions**: Reuse for similar pages
4. **Parallel page fetching**: Process multiple URLs concurrently
5. **Browser instance pooling**: Reuse Chromium instances
6. **Batch LLM calls**: Send multiple observations in one request
7. **Optimize selectors**: More specific = faster extraction
8. **Limit scroll repetitions**: Each scroll adds 800ms+ delay

### Current Limitations

- Sequential page processing (not parallel)
- New browser instance per request
- No LLM response caching
- No request throttling/rate limiting
- Single-threaded worker

### Future Enhancements

- Redis caching for LLM decisions
- Bull queue for background jobs
- Playwright context reuse
- Distributed worker pool
- GraphQL API for real-time updates
- Webhook notifications on completion

---

## Code Reference Summary

| Feature | File | Function/Lines |
|---------|------|----------------|
| Main Orchestration | `lib/agent.ts` | `scrapeProducts()` :7-63 |
| LLM Decision Engine | `lib/brain.ts` | `askClaudeForDecision()` :3-43 |
| HTTP Fetcher | `lib/tools.ts` | `HttpFetcher.fetch()` :10-23 |
| CSS Extraction | `lib/tools.ts` | `extractProductsHTML()` :43-61 |
| JSON-LD Parser | `lib/agent.ts` | `parseJsonLd()` :65-92 |
| Pagination Discovery | `lib/agent.ts` | `findNextLinks()` :94-105 |
| Deduplication | `lib/agent.ts` | `merge()` :107-113 |
| Browser Worker | `worker/browser-worker.ts` | `handle()` :11-27 |
| API Endpoint | `app/api/scrape/route.ts` | `POST()` :13-29 |
| Versioned API (v1) | `app/api/v1/scrape/route.ts` | `POST()` :43-152 |
| Streaming API | `app/api/scrape/stream/route.ts` | SSE endpoint |
| Cron Scheduler | `lib/cron/scheduler.ts` | `runScheduledScrapes()` :6-67 |
| Product Schema | `lib/schemas.ts` | `Product` :3-13 |
| Decision Schema | `lib/schemas.ts` | `AgentDecision` :58-61 |
| Database Schema | `lib/db/schema.ts` | All tables :5-140 |
| Authentication | `lib/auth/index.ts` | NextAuth config |
| Export Utilities | `lib/utils/export.ts` | JSON/CSV functions |
| UI Form | `components/scraper-form.tsx` | Component :9-35 |
| Results Table | `components/results-table.tsx` | Component :2-26 |
| Log Viewer | `components/log-view.tsx` | Component :1-7 |
| Export Buttons | `components/export-buttons.tsx` | Component |
| Dashboard | `app/dashboard/page.tsx` | Server Component :107-134 |
| Job Details | `app/dashboard/jobs/[id]/page.tsx` | Server Component |
| Scheduled Scrapes UI | `app/dashboard/scheduled/page.tsx` | Server Component |
| Streaming Hook | `lib/hooks/use-scrape-stream.ts` | `useScrapeStream()` :8-105 |
| Middleware | `middleware.ts` | Route protection |

---

### 22. User Authentication & Session Management
**Purpose**: Secure user authentication and authorization for job management.

**Implementation**: `lib/auth/index.ts`, `app/api/auth/[...nextauth]/route.ts`

**Features**:
- NextAuth.js 5.0 integration for authentication
- Drizzle ORM adapter for database session storage
- Email verification support
- OAuth provider support (extensible)
- Protected route middleware (`middleware.ts`)
- Anonymous scraping support (no login required)

**Database Tables**:
- **users**: User profiles with email and verification status
- **accounts**: OAuth provider linkage
- **verification_tokens**: Email verification workflow

**Protected Routes**:
- `/dashboard/*` - Requires authentication
- `/api/v1/scrape` - Optional auth (creates jobs if authenticated)

**Session Management**:
- Secure cookie-based sessions
- NEXTAUTH_SECRET for encryption
- NEXTAUTH_URL for callback configuration

---

### 23. Job Tracking & History
**Purpose**: Persistent storage and management of scraping jobs.

**Implementation**: `lib/db/schema.ts` (scrapingJobs table), `app/dashboard/page.tsx`

**Job Lifecycle**:
1. **pending** - Job created, waiting to start
2. **running** - Currently executing
3. **completed** - Successfully finished
4. **failed** - Error occurred

**Tracked Metadata**:
- User ID (owner)
- Target URL and goal
- Status and timestamps (created, started, completed)
- Configuration (maxTotalPages, browserEnabled)
- Progress metrics (pages processed, products found)
- Error messages for failed jobs
- Complete log history

**Database Schema**:
```typescript
{
  id: uuid,
  userId: uuid,
  url: text,
  goal: text,
  status: 'pending' | 'running' | 'completed' | 'failed',
  startedAt: timestamp,
  completedAt: timestamp,
  error: text,
  pagesProcessed: number,
  productsFound: number,
  logs: string[],
  config: {
    maxTotalPages?: number,
    browserEnabled?: boolean
  }
}
```

**Indexes for Performance**:
- User ID index (fast user-specific queries)
- Status index (filter by job state)
- Created date index (chronological sorting)

---

### 24. Scheduled Scraping & Cron Jobs
**Purpose**: Automated recurring scrapes on a schedule.

**Implementation**: `lib/cron/scheduler.ts`, `lib/db/schema.ts` (scheduledScrapes table)

**Features**:
- Flexible schedule patterns (cron-like expressions)
- Enable/disable toggle without deletion
- Automatic next run calculation
- Job creation for each execution
- User-specific schedules (multi-tenant)
- Configuration per schedule

**Supported Schedule Patterns**:
- `@hourly` - Every hour
- `@daily` - Every day
- `@weekly` - Every week
- `@monthly` - Every month
- `every N hours` - Custom hourly interval
- `every N days` - Custom daily interval
- `every N weeks` - Custom weekly interval

**Schedule Configuration**:
```typescript
{
  id: uuid,
  userId: uuid,
  name: string,
  url: text,
  goal: text,
  schedule: string, // cron expression
  enabled: boolean,
  lastRunAt: timestamp,
  nextRunAt: timestamp,
  config: {
    maxTotalPages?: number,
    browserEnabled?: boolean,
    notifyOnComplete?: boolean
  }
}
```

**Execution Endpoint**: `POST /api/cron/scheduled-scrapes`
- Called by cron service (Vercel Cron, external scheduler)
- Processes all due schedules
- Creates tracking jobs for each run
- Updates next run timestamps
- Handles errors gracefully

**Example Usage**:
```typescript
// Create scheduled scrape via UI
{
  name: "Daily Product Sync",
  url: "https://example.com/products",
  goal: "Extract all products",
  schedule: "@daily",
  config: { maxTotalPages: 20 }
}
```

**UI**: `app/dashboard/scheduled/page.tsx` - Manage schedules

---

### 25. Versioned RESTful API (v1)
**Purpose**: Stable, versioned API for external integrations.

**Implementation**: `app/api/v1/scrape/route.ts`

**Endpoint**: `POST /api/v1/scrape`

**Request Schema** (Zod validated):
```json
{
  "url": "https://example.com/products",
  "goal": "Extract product listings",
  "config": {
    "maxTotalPages": 12,
    "browserEnabled": false
  }
}
```

**Response Schema**:
```json
{
  "success": true,
  "data": {
    "jobId": "uuid-here",
    "products": [...],
    "stats": {
      "pagesProcessed": 5,
      "productsFound": 50,
      "duration": 12500
    }
  },
  "meta": {
    "version": "v1",
    "timestamp": "2025-01-15T10:30:00Z"
  }
}
```

**Error Response**:
```json
{
  "success": false,
  "error": {
    "code": "VALIDATION_ERROR" | "INTERNAL_ERROR",
    "message": "Description",
    "details": {...}
  },
  "meta": {
    "version": "v1",
    "timestamp": "..."
  }
}
```

**Features**:
- Structured error codes for programmatic handling
- Comprehensive stats tracking
- Optional authentication (creates job if logged in)
- Version metadata for API stability
- Zod validation with detailed error messages

**Runtime Configuration**:
- Node.js runtime (not Edge)
- Preferred region: "home"
- Max pages: configurable via request

**Authentication**: Optional
- Authenticated: Creates job in database
- Anonymous: Returns results without persistence

---

### 26. Real-Time Streaming API
**Purpose**: Live progress updates during scraping.

**Implementation**: `app/api/scrape/stream/route.ts`, `lib/hooks/use-scrape-stream.ts`

**Protocol**: Server-Sent Events (SSE)

**Event Types**:

1. **job-created**
```
event: job-created
data: {"jobId": "uuid"}
```

2. **logs**
```
event: logs
data: {"logs": ["Fetch HTTP: ...", "Decision: ..."]}
```

3. **progress**
```
event: progress
data: {"products": 25}
```

4. **complete**
```
event: complete
data: {"products": [...], "logs": [...]}
```

5. **error**
```
event: error
data: {"message": "Error description"}
```

**React Hook Integration**:
```typescript
const { startScrape, isStreaming, logs, products, jobId, error } = useScrapeStream();

// Start streaming scrape
await startScrape(url, goal);

// Real-time state updates
console.log(logs); // Live log updates
console.log(products); // Products as discovered
console.log(isStreaming); // Loading state
```

**Benefits**:
- Immediate feedback on long-running operations
- Progressive product loading
- Real-time error notifications
- Better UX for users watching progress

---

### 27. Product Data Export System
**Purpose**: Export scraped data in multiple formats.

**Implementation**: `lib/utils/export.ts`, `components/export-buttons.tsx`

**Export Formats**:

**JSON Export**:
- Pretty-printed with 2-space indentation
- Complete product data with all fields
- Preserves nested structures
- MIME type: `application/json`

**CSV Export**:
- Standard CSV with headers
- Proper escaping (commas, quotes, newlines)
- Flattened breadcrumbs (joined with " > ")
- Boolean values as Yes/No
- MIME type: `text/csv`

**CSV Fields**:
- URL, Title, Price, Currency
- In Stock, SKU, Image
- Breadcrumbs

**Client-Side Export**:
```typescript
import { exportToJSON, exportToCSV, downloadFile } from '@/lib/utils/export';

// JSON export
const json = exportToJSON(products);
downloadFile(json, 'products.json', 'application/json');

// CSV export
const csv = exportToCSV(products);
downloadFile(csv, 'products.csv', 'text/csv');
```

**Server-Side Export API**: `GET /api/export/[jobId]?format=json|csv`
- Exports historical job results
- Authentication required
- Automatic file download
- Format query parameter (json or csv)

**Features**:
- No server storage (direct browser download)
- Instant generation
- Proper escaping for CSV safety
- Preserves all product metadata

---

### 28. Database Architecture & ORM
**Purpose**: Type-safe database operations with PostgreSQL.

**Implementation**: `lib/db/index.ts`, `lib/db/schema.ts`

**Technology Stack**:
- **ORM**: Drizzle ORM (type-safe, lightweight)
- **Database**: PostgreSQL via Neon Serverless
- **Connection**: WebSocket-based serverless connection

**Tables**:

1. **users** - User accounts
2. **accounts** - OAuth provider linkage
3. **verification_tokens** - Email verification
4. **scraping_jobs** - Job tracking
5. **scraped_products** - Extracted product data
6. **scheduled_scrapes** - Recurring job configuration

**Relationships**:
- Users → Jobs (one-to-many)
- Users → Scheduled Scrapes (one-to-many)
- Jobs → Products (one-to-many)
- Users → Accounts (one-to-many)

**Cascade Deletion**:
- Delete user → Deletes all jobs, products, accounts, schedules
- Delete job → Deletes all associated products
- Ensures data consistency

**Performance Indexes**:
- User ID on jobs, schedules (fast user queries)
- Job ID on products (fast product retrieval)
- Status on jobs (filter by state)
- Created date on jobs (chronological sorting)
- Enabled status on schedules (filter active)
- Next run time on schedules (cron processing)

**Drizzle Commands**:
```bash
npm run db:generate  # Generate migrations
npm run db:migrate   # Run migrations
npm run db:push      # Push schema changes
npm run db:studio    # Visual database browser
```

**Configuration**: `drizzle.config.ts`
- Connection string from DATABASE_URL
- Migration output directory
- Schema file locations

---

### 29. Middleware & Route Protection
**Purpose**: Protect authenticated routes and manage sessions.

**Implementation**: `middleware.ts`

**Features**:
- Route-based authentication checks
- Automatic redirects for unauthenticated users
- Session verification via NextAuth
- Public route exceptions

**Protected Route Patterns**:
- `/dashboard/*` - User dashboard and history
- `/dashboard/jobs/*` - Job detail pages
- `/dashboard/scheduled/*` - Schedule management

**Public Routes**:
- `/` - Homepage with scraper form
- `/api/scrape` - Anonymous scraping allowed
- `/api/v1/scrape` - Optional authentication
- `/auth/*` - Sign in/sign up pages

**Redirect Behavior**:
- Unauthenticated access to protected route → `/auth/signin`
- Post-login redirect to originally requested page

---

### 30. Job Detail View & Product Management
**Purpose**: Detailed view of individual scraping jobs.

**Implementation**: `app/dashboard/jobs/[id]/page.tsx`

**Features**:
- Job metadata display (URL, goal, status)
- Performance statistics
  - Pages processed
  - Products found
  - Duration (seconds)
- Complete log history
- Product listing table
- Export functionality (JSON/CSV)
- Error messages for failed jobs
- Timestamps (created, started, completed)

**Access Control**:
- User can only view their own jobs
- Authenticated access required
- 404 for non-existent or unauthorized jobs

**UI Components**:
- Status badge with icon
- Metrics cards
- Collapsible log viewer
- Sortable product table
- Export buttons

---

**Last Updated**: 2025-10-19
**Version**: 1.0.0
**Maintainer**: FetchPilot Team
